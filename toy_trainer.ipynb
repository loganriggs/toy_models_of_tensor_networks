{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Toy Transformer Training Script with NanoGPT Speedrun Improvements\n",
    "Incorporates key improvements from modded-nanogpt for training small experimental models\n",
    "Now with real data streaming from HuggingFace datasets\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "from einops import rearrange, einsum\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Literal\n",
    "import time\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Muon Optimizer from modded-nanogpt\n",
    "# ============================================================================\n",
    "\n",
    "@torch.compile\n",
    "def zeroth_power_via_newtonschulz5(G, steps=5, eps=1e-7):\n",
    "    \"\"\"Newton-Schulz iteration for orthogonalization used in Muon.\"\"\"\n",
    "    assert len(G.shape) == 2\n",
    "    a, b, c = (3.4445, -4.7750, 2.0315)\n",
    "    X = G.bfloat16() / (G.norm() + eps)\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.T\n",
    "        B = b * A + c * A @ A\n",
    "        X = a * X + B @ X\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    return X.to(G.dtype)\n",
    "\n",
    "\n",
    "class Muon(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Muon optimizer from modded-nanogpt\n",
    "    Memory efficient, ~1.5x better sample efficiency than Adam\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=1e-3, momentum=0.95, ns_steps=5):\n",
    "        defaults = dict(lr=lr, momentum=momentum, ns_steps=ns_steps)\n",
    "        super().__init__(params, defaults)\n",
    "        \n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            momentum = group['momentum']\n",
    "            ns_steps = group['ns_steps']\n",
    "            \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                    \n",
    "                grad = p.grad\n",
    "                state = self.state[p]\n",
    "                \n",
    "                if len(state) == 0:\n",
    "                    state['momentum_buffer'] = torch.zeros_like(grad)\n",
    "                \n",
    "                buf = state['momentum_buffer']\n",
    "                buf.mul_(momentum).add_(grad)\n",
    "                \n",
    "                # Orthogonalize momentum buffer\n",
    "                if len(grad.shape) >= 2:\n",
    "                    grad_2d = buf.view(buf.shape[0], -1)\n",
    "                    orthogonal_grad = zeroth_power_via_newtonschulz5(grad_2d, steps=ns_steps)\n",
    "                    buf = orthogonal_grad.view_as(buf)\n",
    "                \n",
    "                p.data.add_(buf, alpha=-lr)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Core Components\n",
    "# ============================================================================\n",
    "\n",
    "class Rotary(nn.Module):\n",
    "    \"\"\"Rotary positional embeddings\"\"\"\n",
    "    def __init__(self, dim, max_seq_len=2048, base=10000):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self._build_cache()\n",
    "        \n",
    "    def _build_cache(self):\n",
    "        t = torch.arange(self.max_seq_len, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        cos_cached = emb.cos()[None, :, None, :]\n",
    "        sin_cached = emb.sin()[None, :, None, :]\n",
    "        self.register_buffer('cos_cached', cos_cached)\n",
    "        self.register_buffer('sin_cached', sin_cached)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "        cos = self.cos_cached[:, :seq_len]\n",
    "        sin = self.sin_cached[:, :seq_len]\n",
    "        \n",
    "        x_even = x[..., 0::2]\n",
    "        x_odd = x[..., 1::2]\n",
    "        \n",
    "        x_rotated = torch.stack(\n",
    "            [-x_odd, x_even],\n",
    "            dim=-1\n",
    "        ).flatten(-2)\n",
    "        \n",
    "        return x * cos + x_rotated * sin\n",
    "\n",
    "\n",
    "class Component(nn.Module):\n",
    "    \"\"\"Base component class\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class Mask(nn.Module):\n",
    "    \"\"\"Masking for attention patterns\"\"\"\n",
    "    def __init__(self, n_ctx, mask_type='causal'):\n",
    "        super().__init__()\n",
    "        self.n_ctx = n_ctx\n",
    "        self.mask_type = mask_type\n",
    "        \n",
    "        if mask_type == 'causal':\n",
    "            mask = torch.tril(torch.ones(n_ctx, n_ctx))\n",
    "        else:  # no mask\n",
    "            mask = torch.ones(n_ctx, n_ctx)\n",
    "        \n",
    "        self.register_buffer('mask', mask)\n",
    "        \n",
    "    def forward(self, scores):\n",
    "        seq_len = scores.shape[-1]\n",
    "        mask = self.mask[:seq_len, :seq_len]\n",
    "        return scores * mask\n",
    "\n",
    "\n",
    "class QuadraticAttention(Component):\n",
    "    \"\"\"Attention using quadratic scoring function instead of softmax\"\"\"\n",
    "    def __init__(self, d_model: int, n_head: int, n_ctx: int, \n",
    "                 mask: str = 'causal', scale: int = 1, \n",
    "                 norm: bool = True, bias: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        self.d_head = d_model // n_head\n",
    "        self.n_head = n_head\n",
    "        self.n_ctx = n_ctx\n",
    "        self.d_model = d_model\n",
    "        self.scale = scale\n",
    "        \n",
    "        self.rotary = Rotary(self.d_head, n_ctx)\n",
    "        self.norm = nn.RMSNorm(self.d_head) if norm else nn.Identity()\n",
    "        self.mask = Mask(n_ctx, mask)\n",
    "        \n",
    "        # Initialize QKV projections\n",
    "        self.q = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.k = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.v = nn.Linear(d_model, d_model, bias=bias)\n",
    "        \n",
    "        # Zero-initialize output projection (muP-like)\n",
    "        self.o = nn.Linear(d_model, d_model, bias=False)\n",
    "        init.zeros_(self.o.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        q, k, v = [rearrange(op(x), '... (n_head d_head) -> ... n_head d_head', \n",
    "                            n_head=self.n_head) for op in [self.q, self.k, self.v]]\n",
    "        \n",
    "        # Apply rotary embeddings and normalization\n",
    "        q, k = self.rotary(self.norm(q)), self.rotary(self.norm(k))\n",
    "        \n",
    "        # Quadratic scoring function\n",
    "        scores = einsum(q, k, \"... seq_q n_head d_head, ... seq_k n_head d_head -> ... n_head seq_q seq_k\")\n",
    "        pattern = self.mask((scores / self.d_head).square())\n",
    "        \n",
    "        # Aggregate values\n",
    "        z = einsum(pattern, v, \"... n_head seq_q seq_k, ... seq_k n_head d_head -> ... seq_q n_head d_head\")\n",
    "        z = rearrange(z, '... seq n_head d_head -> ... seq (n_head d_head)')\n",
    "        \n",
    "        return x + self.o(z) * self.scale\n",
    "\n",
    "\n",
    "class ReLUSquared(nn.Module):\n",
    "    \"\"\"ReLUÂ² activation from modded-nanogpt\"\"\"\n",
    "    def forward(self, x):\n",
    "        return F.relu(x).square()\n",
    "\n",
    "\n",
    "class BilinearLayer(nn.Module):\n",
    "    \"\"\"Bilinear layer: element-wise product of two linear projections\n",
    "    Similar to SwiGLU but without the gating nonlinearity\"\"\"\n",
    "    def __init__(self, d_model, d_hidden=None, bias=True):\n",
    "        super().__init__()\n",
    "        d_hidden = d_hidden or 4 * d_model\n",
    "        \n",
    "        # Two parallel projections to hidden dimension\n",
    "        self.proj1 = nn.Linear(d_model, d_hidden, bias=bias)\n",
    "        self.proj2 = nn.Linear(d_model, d_hidden, bias=bias)\n",
    "        \n",
    "        # Output projection\n",
    "        self.down = nn.Linear(d_hidden, d_model, bias=bias)\n",
    "        \n",
    "        # Zero-initialize down projection for stability\n",
    "        init.zeros_(self.down.weight)\n",
    "        if bias:\n",
    "            init.zeros_(self.down.bias)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # Compute two parallel projections and multiply element-wise\n",
    "        hidden = self.proj1(x) * self.proj2(x)\n",
    "        return self.down(hidden)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Single transformer block with quadratic attention and bilinear layer\"\"\"\n",
    "    def __init__(self, d_model, n_head, n_ctx, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.attn = QuadraticAttention(d_model, n_head, n_ctx)\n",
    "        self.bilinear = BilinearLayer(d_model)\n",
    "        self.ln1 = nn.RMSNorm(d_model)\n",
    "        self.ln2 = nn.RMSNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.attn(self.ln1(x)))\n",
    "        x = x + self.dropout(self.bilinear(self.ln2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Model Configurations\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    model_type: Literal['attention_only_1L', 'attention_only_2L', 'transformer_1L', 'transformer_2L']\n",
    "    vocab_size: int = 50257  # GPT-2 tokenizer vocab size\n",
    "    d_model: int = 768\n",
    "    n_head: int = 12\n",
    "    n_ctx: int = 1024\n",
    "    dropout: float = 0.0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        assert self.d_model % self.n_head == 0\n",
    "\n",
    "\n",
    "class ToyTransformer(nn.Module):\n",
    "    \"\"\"Flexible toy transformer supporting different configurations\"\"\"\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.embed = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.pos_embed = nn.Embedding(config.n_ctx, config.d_model)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Build layers based on model type\n",
    "        layers = []\n",
    "        if config.model_type == 'attention_only_1L':\n",
    "            layers.append(QuadraticAttention(config.d_model, config.n_head, config.n_ctx))\n",
    "            layers.append(nn.RMSNorm(config.d_model))\n",
    "        elif config.model_type == 'attention_only_2L':\n",
    "            for _ in range(2):\n",
    "                layers.append(QuadraticAttention(config.d_model, config.n_head, config.n_ctx))\n",
    "                layers.append(nn.RMSNorm(config.d_model))\n",
    "        elif config.model_type == 'transformer_1L':\n",
    "            layers.append(TransformerBlock(config.d_model, config.n_head, config.n_ctx, config.dropout))\n",
    "        elif config.model_type == 'transformer_2L':\n",
    "            for _ in range(2):\n",
    "                layers.append(TransformerBlock(config.d_model, config.n_head, config.n_ctx, config.dropout))\n",
    "        \n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        \n",
    "        # Output head\n",
    "        self.ln_f = nn.RMSNorm(config.d_model)\n",
    "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying\n",
    "        self.head.weight = self.embed.weight\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"Generate text autoregressively\"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop context if needed\n",
    "            idx_cond = idx if idx.size(1) <= self.config.n_ctx else idx[:, -self.config.n_ctx:]\n",
    "            \n",
    "            # Get predictions\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Optional top-k sampling\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            \n",
    "            # Sample from distribution\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append to sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "            \n",
    "        return idx\n",
    "        b, t = idx.shape\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=idx.device).unsqueeze(0)\n",
    "        \n",
    "        # Token + position embeddings\n",
    "        x = self.embed(idx) + self.pos_embed(pos)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Forward through layers\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, (QuadraticAttention, TransformerBlock)):\n",
    "                x = layer(x)\n",
    "            else:  # RMSNorm layers for attention-only models\n",
    "                x = layer(x)\n",
    "        \n",
    "        # Output projection\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        # Calculate loss if targets provided\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Data Loading\n",
    "# ============================================================================\n",
    "\n",
    "class StreamingTextDataset:\n",
    "    \"\"\"Streaming dataset that tokenizes text on-the-fly\"\"\"\n",
    "    def __init__(self, dataset_name='HuggingFaceFW/fineweb', split='train', \n",
    "                 tokenizer_name='gpt2', seq_length=1024, subset='sample-10BT',\n",
    "                 validation_ratio=0.001, seed=42):\n",
    "        from transformers import AutoTokenizer\n",
    "        from datasets import load_dataset\n",
    "        import hashlib\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.seq_length = seq_length\n",
    "        self.validation_ratio = validation_ratio\n",
    "        self.is_validation = (split == 'validation')\n",
    "        \n",
    "        # Load streaming dataset\n",
    "        # For FineWeb, we only have 'train' split available\n",
    "        actual_split = 'train' if 'fineweb' in dataset_name.lower() else split\n",
    "        \n",
    "        if subset:\n",
    "            self.dataset = load_dataset(dataset_name, subset, split=actual_split, streaming=True)\n",
    "        else:\n",
    "            self.dataset = load_dataset(dataset_name, split=actual_split, streaming=True)\n",
    "        \n",
    "        # For datasets with only train split, we create our own train/val split\n",
    "        # using a hash-based deterministic split\n",
    "        if 'fineweb' in dataset_name.lower() or actual_split == 'train':\n",
    "            self.dataset = self.dataset.shuffle(seed=seed, buffer_size=10000)\n",
    "            \n",
    "            # Filter to create train/validation split based on hash of content\n",
    "            def should_include(example):\n",
    "                # Use hash of text to deterministically split data\n",
    "                text = example.get('text', example.get('content', ''))\n",
    "                hash_val = int(hashlib.md5(text.encode()).hexdigest(), 16)\n",
    "                is_val_sample = (hash_val % int(1/validation_ratio)) == 0\n",
    "                \n",
    "                # Include if: (we want validation and this is validation) or \n",
    "                #            (we want train and this is not validation)\n",
    "                return is_val_sample == self.is_validation\n",
    "            \n",
    "            self.dataset = self.dataset.filter(should_include)\n",
    "            \n",
    "        # Create iterator\n",
    "        self.iterator = iter(self.dataset)\n",
    "        self.token_buffer = []\n",
    "        \n",
    "    def get_batch(self, batch_size, device='cuda'):\n",
    "        \"\"\"Get a batch of tokenized sequences\"\"\"\n",
    "        batch_tokens = []\n",
    "        \n",
    "        while len(batch_tokens) < batch_size:\n",
    "            # Refill token buffer if needed\n",
    "            while len(self.token_buffer) < self.seq_length + 1:\n",
    "                try:\n",
    "                    # Get next text sample\n",
    "                    sample = next(self.iterator)\n",
    "                    text = sample.get('text', sample.get('content', ''))\n",
    "                    \n",
    "                    # Tokenize and add to buffer\n",
    "                    tokens = self.tokenizer.encode(text, truncation=False, add_special_tokens=False)\n",
    "                    self.token_buffer.extend(tokens)\n",
    "                except StopIteration:\n",
    "                    # Restart dataset if we run out\n",
    "                    self.iterator = iter(self.dataset)\n",
    "                    if len(self.token_buffer) == 0:  # Prevent infinite loop\n",
    "                        # Add some padding tokens if completely empty\n",
    "                        self.token_buffer = [self.tokenizer.eos_token_id] * (self.seq_length + 1)\n",
    "                        break\n",
    "            \n",
    "            # Extract sequence from buffer\n",
    "            if len(self.token_buffer) >= self.seq_length + 1:\n",
    "                seq = self.token_buffer[:self.seq_length + 1]\n",
    "                batch_tokens.append(seq)\n",
    "                # Remove processed tokens (with some overlap to maintain context)\n",
    "                self.token_buffer = self.token_buffer[self.seq_length:]\n",
    "        \n",
    "        # Convert to tensors\n",
    "        batch = torch.tensor(batch_tokens, dtype=torch.long, device=device)\n",
    "        x = batch[:, :-1]  # Input sequences\n",
    "        y = batch[:, 1:]   # Target sequences (shifted by 1)\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Training Script\n",
    "# ============================================================================\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, config, device='cuda'):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        \n",
    "        # Setup optimizer (Muon)\n",
    "        self.optimizer = Muon(\n",
    "            self.model.parameters(),\n",
    "            lr=config.learning_rate,\n",
    "            momentum=config.momentum\n",
    "        )\n",
    "        \n",
    "        # Learning rate schedule with warmup and cooldown\n",
    "        self.iteration = 0\n",
    "        \n",
    "    def get_lr(self):\n",
    "        \"\"\"Cosine learning rate schedule with warmup\"\"\"\n",
    "        if self.iteration < self.config.warmup_iters:\n",
    "            return self.config.learning_rate * self.iteration / self.config.warmup_iters\n",
    "        if self.iteration > self.config.lr_decay_iters:\n",
    "            return self.config.min_lr\n",
    "        decay_ratio = (self.iteration - self.config.warmup_iters) / (self.config.lr_decay_iters - self.config.warmup_iters)\n",
    "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "        return self.config.min_lr + coeff * (self.config.learning_rate - self.config.min_lr)\n",
    "    \n",
    "    def train_step(self, x, y):\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        # Set learning rate\n",
    "        lr = self.get_lr()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "        # Forward pass\n",
    "        logits, loss = self.model(x, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        if self.config.grad_clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_clip)\n",
    "            \n",
    "        # Optimizer step\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.iteration += 1\n",
    "        \n",
    "        return loss.item(), lr\n",
    "    \n",
    "    def evaluate(self, dataloader, max_batches=50):\n",
    "        \"\"\"Evaluate model on validation set\"\"\"\n",
    "        self.model.eval()\n",
    "        losses = []\n",
    "        with torch.no_grad():\n",
    "            for i, (x, y) in enumerate(dataloader):\n",
    "                if i >= max_batches:\n",
    "                    break\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                _, loss = self.model(x, y)\n",
    "                losses.append(loss.item())\n",
    "        self.model.train()\n",
    "        return np.mean(losses)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    # Model\n",
    "    model_config: ModelConfig = None\n",
    "    \n",
    "    # Training\n",
    "    batch_size: int = 64\n",
    "    learning_rate: float = 3e-3\n",
    "    momentum: float = 0.95\n",
    "    min_lr: float = 3e-4  # Don't decay to 0, following speedrun\n",
    "    warmup_iters: int = 100\n",
    "    lr_decay_iters: int = 5000\n",
    "    max_iters: int = 5000\n",
    "    grad_clip: float = 1.0\n",
    "    \n",
    "    # Logging\n",
    "    eval_interval: int = 100\n",
    "    log_interval: int = 10\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Configure model - using real tokenizer vocab size now\n",
    "    model_config = ModelConfig(\n",
    "        model_type='transformer_1L',  # Change this to experiment with different architectures\n",
    "        vocab_size=50257,  # GPT-2 tokenizer size\n",
    "        d_model=512,  # Moderate size for experiments\n",
    "        n_head=8,\n",
    "        n_ctx=512,  # Shorter context for faster training\n",
    "        dropout=0.1\n",
    "    )\n",
    "    \n",
    "    training_config = TrainingConfig(\n",
    "        model_config=model_config,\n",
    "        batch_size=16,  # Adjust based on GPU memory\n",
    "        learning_rate=3e-3,\n",
    "        max_iters=10000,\n",
    "        eval_interval=500,\n",
    "        log_interval=50\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    model = ToyTransformer(model_config)\n",
    "    print(f\"Model type: {model_config.model_type}\")\n",
    "    print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    print(\"Initializing datasets...\")\n",
    "    train_dataset = StreamingTextDataset(\n",
    "        dataset_name='HuggingFaceFW/fineweb',  # Or 'openwebtext' or 'EleutherAI/pile'\n",
    "        subset='sample-10BT',  # 10B token sample\n",
    "        split='train',  # Will automatically exclude validation samples\n",
    "        seq_length=model_config.n_ctx,\n",
    "        validation_ratio=0.001  # 0.1% for validation\n",
    "    )\n",
    "    \n",
    "    val_dataset = StreamingTextDataset(\n",
    "        dataset_name='HuggingFaceFW/fineweb',\n",
    "        subset='sample-10BT',\n",
    "        split='validation',  # Will automatically filter to validation samples only\n",
    "        seq_length=model_config.n_ctx,\n",
    "        validation_ratio=0.001  # Same ratio to ensure consistent split\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(model, training_config)\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "    for iter in range(training_config.max_iters):\n",
    "        # Get batch of real data\n",
    "        x, y = train_dataset.get_batch(training_config.batch_size)\n",
    "        \n",
    "        # Train step\n",
    "        loss, lr = trainer.train_step(x, y)\n",
    "        \n",
    "        # Logging\n",
    "        if iter % training_config.log_interval == 0:\n",
    "            print(f\"Iter {iter}: loss={loss:.4f}, lr={lr:.6f}\")\n",
    "            \n",
    "        # Evaluation\n",
    "        if iter % training_config.eval_interval == 0 and iter > 0:\n",
    "            val_losses = []\n",
    "            for _ in range(20):  # Evaluate on 20 batches\n",
    "                x_val, y_val = val_dataset.get_batch(training_config.batch_size)\n",
    "                _, val_loss = model(x_val, y_val)\n",
    "                val_losses.append(val_loss.item())\n",
    "            val_loss = np.mean(val_losses)\n",
    "            print(f\"Validation loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # Generate sample text\n",
    "            model.eval()\n",
    "            context = torch.zeros((1, 1), dtype=torch.long, device='cuda')\n",
    "            generated = model.generate(context, max_new_tokens=100, temperature=0.8)\n",
    "            print(f\"Sample generation: {train_dataset.tokenizer.decode(generated[0].tolist())}\")\n",
    "            model.train()\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), f'toy_transformer_{model_config.model_type}.pt')\n",
    "    print(f\"Model saved to toy_transformer_{model_config.model_type}.pt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: transformer_1L\n",
      "Parameters: 30,195,776\n",
      "Initializing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2de372f7ba4ec38e49d79ed076cd64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848a49de945f40f69b7692fdbc3dc9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 566\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    557\u001b[39m val_dataset = StreamingTextDataset(\n\u001b[32m    558\u001b[39m     dataset_name=\u001b[33m'\u001b[39m\u001b[33mHuggingFaceFW/fineweb\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    559\u001b[39m     subset=\u001b[33m'\u001b[39m\u001b[33msample-10BT\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    562\u001b[39m     validation_ratio=\u001b[32m0.001\u001b[39m  \u001b[38;5;66;03m# Same ratio to ensure consistent split\u001b[39;00m\n\u001b[32m    563\u001b[39m )\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# Create trainer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m trainer = \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[32m    569\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 438\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, model, config, device)\u001b[39m\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, device=\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m     \u001b[38;5;28mself\u001b[39m.device = device\n\u001b[32m    440\u001b[39m     \u001b[38;5;28mself\u001b[39m.config = config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/toy_models_of_tensor_networks/toy_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1369\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1366\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1367\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1369\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/toy_models_of_tensor_networks/toy_env/lib/python3.13/site-packages/torch/nn/modules/module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    932\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    933\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    938\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    939\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/toy_models_of_tensor_networks/toy_env/lib/python3.13/site-packages/torch/nn/modules/module.py:955\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    951\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    952\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    953\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    956\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    958\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/toy_models_of_tensor_networks/toy_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1355\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1348\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1349\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1350\u001b[39m             device,\n\u001b[32m   1351\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1352\u001b[39m             non_blocking,\n\u001b[32m   1353\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1354\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1361\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/toy_models_of_tensor_networks/toy_env/lib/python3.13/site-packages/torch/cuda/__init__.py:412\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCUDA_MODULE_LOADING\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os.environ:\n\u001b[32m    411\u001b[39m     os.environ[\u001b[33m\"\u001b[39m\u001b[33mCUDA_MODULE_LOADING\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mLAZY\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[32m    414\u001b[39m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[32m    416\u001b[39m _tls.is_initializing = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero."
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: False\n",
      "CUDA is not available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "cuda = torch.cuda.is_available()\n",
    "print(f\"CUDA is available: {cuda}\")\n",
    "\n",
    "if cuda:\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu126\n",
      "CUDA built with PyTorch: 12.6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA built with PyTorch: {torch.version.cuda}\")b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(f\"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set')}\")\n",
    "print(f\"LD_LIBRARY_PATH: {os.environ.get('LD_LIBRARY_PATH', 'Not set')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Try unsetting it\n",
    "if 'CUDA_VISIBLE_DEVICES' in os.environ:\n",
    "    del os.environ['CUDA_VISIBLE_DEVICES']\n",
    "\n",
    "# Or try setting it explicitly to 0\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# Then reimport torch\n",
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
